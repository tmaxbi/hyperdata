# Default values for hyperdata-system.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  fullPath: biqa.tmax.com/hyperdata20.5_rel/hyperdata20.5_system/system_management:20230829_f68e8bf2
  ## repository: harbor:31234/hyperdata_v20.5_system/system_management
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  ## tag: 5f091111

pvc:
  name: tibero-pvc-db
  mountPath: /db

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
# fsGroup: 2000

securityContext:
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000

service:
  type: NodePort
  port: 8100

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  limits:
    cpu: "1"
    memory: "1Gi"
  requests:
    cpu: "1"
    memory: "1Gi"

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

prometheus:
  serviceName: prometheus-kube-prometheus-prometheus
  namespace: monitoring
  port: 9090
## added setting
applicationProperties: "application.yml"

proxy:
  bodysize: 1024m
  # minutes
  timeout: 1800

server:
  port: 8100
  servlet:
    contextPath: /hyperdata
logging:
  config: classpath:logback-production.xml
spring:
  profiles:
    active: prod
  servlet:
    multipart:
      maxFileSize: 10MB
      maxRequestSize: 10MB
  datasource:
    username: hyperdata
    password: tmax
    url: jdbc:tibero:thin:@tiberolocaldns:8629:tibero
    driverClassName: com.tmax.tibero.jdbc.TbDriver
    hikari:
      jdbcUrl: jdbc:tibero:thin:@tiberolocaldns:8629:tibero
      driverClassName: com.tmax.tibero.jdbc.TbDriver
  rabbitmq:
    enable: true
    host: rabbitmq
    port: 5672
    username: Admin
    password: tmaxtower
  mail:
    enable: false
    host: smtp.gmail.com
    port: 587
    properties:
      mail:
        debug: true
        smtp:
          connectionTimeout: 5000
          auth: true
          starttls:
            enable: true
          ssl:
            enable: false
    username: test@gmail.com
    password: PASSWORD
  mvc:
    logRequestDetails: true
    logResolvedException: true

  jpa:
    hibernate:
      naming:
        physicalStrategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
        implicitStrategy: org.hibernate.boot.model.naming.ImplicitNamingStrategyLegacyJpaImpl
    showSql: true
    properties:
      hibernate:
        formatSql: true
    databasePlatform: org.hibernate.dialect.Oracle10gDialect
##### Web Server Config ########################################
ws:
  default: /home/systemmanagement/
  profiles: /home/systemmanagement/profiles/
### TIBERO
tibero:
  ip: tiberolocaldns
  port: 8629
  username: hyperdata
  password: tmax
  sid: tibero
###Kubernetes
kubernetes:
  config:
    type: kubernetes
  meta:
    name:
      #namespace:
      #hyperdata: hyperdata
      rbac:
        clusterRole: spark-cr
        clusterRoleBinding: systemmanagement-project-clusterrolebinding-
        serviceAccount: default
      pvc:
        ozone: data-datanode-0
    replicas: 3

### keycloak ################
keycloak:
  enabled: true
  realm: HyperDataRealm
  authServerUrl: http://keycloak:8888/
  sslRequired: none
  resource: HyperDataLogin
  secret: SECRET
  #credentials:
  #  secret: SECRET
  useResourceRoleMappings: true
  bearerOnly: true
### keycloak end ################

##ozone##
ozone:
  config:
    host: om
    port: 9862

###customAddonWidget###
customAddon:
  enabled: false
  dnsEnabled: false
  externalHost: 128.59.105.24
  externalDetail: https://192.168.6.15:31519/hyperdata-widget/~fdc/sample.html
  externalPort: 80
ml:
  enabled: true
imagePullSecrets: ""


